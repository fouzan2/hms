#!/usr/bin/env python3
"""
Test evaluation components for HMS EEG Classification System.
Tests evaluator, performance metrics, cross-validation, robustness testing, and clinical validation.
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import pandas as pd
from pathlib import Path
from typing import Dict, List
import logging
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_evaluation_imports():
    """Test that all evaluation components can be imported."""
    logger.info("üß™ Testing evaluation imports...")
    
    try:
        from evaluation import (
            ModelEvaluator,
            ClinicalMetrics,
            BiasDetector,
            EvaluationVisualizer,
            PerformanceMetrics,
            CrossValidationManager,
            NoiseRobustnessTester,
            ClinicalAgreementValidator
        )
        logger.info("‚úÖ All evaluation imports successful")
        return True
    except ImportError as e:
        logger.error(f"‚ùå Evaluation import failed: {e}")
        return False


def test_clinical_metrics():
    """Test clinical metrics calculations."""
    logger.info("üß™ Testing clinical metrics...")
    
    try:
        from evaluation import ClinicalMetrics
        
        # Mock configuration
        config = {
            'classes': ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']
        }
        
        # Create clinical metrics calculator
        clinical_metrics = ClinicalMetrics(config)
        
        # Generate synthetic data
        np.random.seed(42)
        n_samples = 100
        y_true = np.random.randint(0, 6, n_samples)
        y_pred = np.random.randint(0, 6, n_samples)
        y_prob = np.random.rand(n_samples, 6)
        
        # Test seizure detection metrics
        seizure_metrics = clinical_metrics.seizure_detection_metrics(y_true, y_pred, y_prob)
        assert isinstance(seizure_metrics, dict)
        assert 'seizure_sensitivity' in seizure_metrics
        assert 'seizure_specificity' in seizure_metrics
        assert 'false_alarm_rate_per_hour' in seizure_metrics
        logger.info(f"‚úÖ Seizure metrics computed: {list(seizure_metrics.keys())}")
        
        # Test periodic discharge metrics
        pd_metrics = clinical_metrics.periodic_discharge_metrics(y_true, y_pred)
        assert isinstance(pd_metrics, dict)
        assert 'LPD_sensitivity' in pd_metrics
        assert 'GPD_sensitivity' in pd_metrics
        logger.info(f"‚úÖ Periodic discharge metrics computed: {list(pd_metrics.keys())}")
        
        # Test rhythmic activity metrics
        ra_metrics = clinical_metrics.rhythmic_activity_metrics(y_true, y_pred)
        assert isinstance(ra_metrics, dict)
        assert 'LRDA_sensitivity' in ra_metrics
        assert 'GRDA_sensitivity' in ra_metrics
        logger.info(f"‚úÖ Rhythmic activity metrics computed: {list(ra_metrics.keys())}")
        
        logger.info("‚úÖ Clinical metrics test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Clinical metrics test failed: {e}")
        return False


def test_model_evaluator():
    """Test model evaluator with mock model and data."""
    logger.info("üß™ Testing model evaluator...")
    
    try:
        from evaluation import ModelEvaluator
        import torch.nn as nn
        
        # Load configuration
        import yaml
        with open('config/config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        # Create simple mock model
        class MockModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.fc = nn.Linear(19*100, 6)  # Smaller input size
                
            def forward(self, eeg, spectrogram):
                batch_size = eeg.shape[0]
                x = eeg.reshape(batch_size, -1)
                logits = self.fc(x)
                return {'logits': logits}
        
        model = MockModel()
        
        # Create evaluator - this tests basic initialization
        evaluator = ModelEvaluator(model, config, device='cpu')
        
        logger.info("‚úÖ Model evaluator initialized successfully")
        logger.info("‚úÖ Model evaluator test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Model evaluator test failed: {e}")
        return False


def test_cross_validation():
    """Test cross-validation functionality."""
    logger.info("üß™ Testing cross-validation...")
    
    try:
        from evaluation import StratifiedKFoldCV
        
        # Just test basic initialization
        cv_strategy = StratifiedKFoldCV(n_splits=3, random_state=42)
        
        logger.info("‚úÖ Cross-validation strategy initialized successfully")
        logger.info("‚úÖ Cross-validation test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Cross-validation test failed: {e}")
        return False


def test_robustness_testing():
    """Test robustness testing components."""
    logger.info("üß™ Testing robustness testing...")
    
    try:
        from evaluation import NoiseRobustnessTester
        import torch.nn as nn
        
        # Create simple mock model
        class MockModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.fc = nn.Linear(100, 6)
                
            def forward(self, x):
                return self.fc(x.reshape(x.shape[0], -1))
        
        model = MockModel()
        
        # Just test initialization
        tester = NoiseRobustnessTester(model, device='cpu')
        
        logger.info("‚úÖ Noise robustness tester initialized successfully")
        logger.info("‚úÖ Robustness testing test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Robustness testing test failed: {e}")
        return False


def test_performance_metrics():
    """Test performance metrics calculations."""
    logger.info("üß™ Testing performance metrics...")
    
    try:
        from evaluation import PerformanceMetrics
        
        # Test basic initialization
        class_names = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']
        metrics_calc = PerformanceMetrics(class_names=class_names)
        
        # Generate synthetic data
        np.random.seed(42)
        n_samples = 100
        y_true = np.random.randint(0, 6, n_samples)
        y_pred = np.random.randint(0, 6, n_samples)
        y_prob = np.random.rand(n_samples, 6)
        y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)
        
        # Test the main computation method
        if hasattr(metrics_calc, 'compute_all_metrics'):
            metrics = metrics_calc.compute_all_metrics(y_true, y_pred, y_prob)
            assert isinstance(metrics, dict)
            logger.info("‚úÖ Performance metrics computed successfully")
        else:
            logger.info("‚úÖ Performance metrics initialized successfully")
        
        logger.info("‚úÖ Performance metrics test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Performance metrics test failed: {e}")
        return False


def test_bias_detection():
    """Test bias detection functionality."""
    logger.info("üß™ Testing bias detection...")
    
    try:
        from evaluation import BiasDetector
        
        # Create configuration
        config = {
            'classes': ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']
        }
        
        # Create bias detector
        bias_detector = BiasDetector(config)
        
        # Generate synthetic data with demographics
        np.random.seed(42)
        n_samples = 100
        predictions = np.random.randint(0, 6, n_samples)
        targets = np.random.randint(0, 6, n_samples)
        
        # Create synthetic demographics
        demographics = pd.DataFrame({
            'age': np.random.randint(18, 80, n_samples),
            'gender': np.random.choice(['M', 'F'], n_samples),
            'recording_condition': np.random.choice(['ICU', 'EMU', 'Outpatient'], n_samples)
        })
        
        # Test bias detection
        bias_metrics = bias_detector.detect_demographic_bias(
            predictions, targets, demographics
        )
        
        assert isinstance(bias_metrics, dict)
        expected_bias_types = ['age_bias', 'gender_bias', 'condition_bias']
        
        for bias_type in expected_bias_types:
            if bias_type in bias_metrics:
                assert isinstance(bias_metrics[bias_type], dict)
                logger.info(f"‚úÖ {bias_type} analysis completed")
        
        logger.info("‚úÖ Bias detection test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Bias detection test failed: {e}")
        return False


def test_evaluation_visualizer():
    """Test evaluation visualization components."""
    logger.info("üß™ Testing evaluation visualizer...")
    
    try:
        from evaluation import EvaluationVisualizer
        import matplotlib.pyplot as plt
        
        # Generate synthetic data
        np.random.seed(42)
        n_samples = 50
        n_classes = 6
        y_true = np.random.randint(0, n_classes, n_samples)
        y_pred = np.random.randint(0, n_classes, n_samples)
        y_prob = np.random.rand(n_samples, n_classes)
        y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)
        
        class_names = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']
        
        # Test confusion matrix plotting
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # Test plotting (don't actually show)
        plt.ioff()  # Turn off interactive mode
        
        try:
            EvaluationVisualizer.plot_confusion_matrix(cm, class_names)
            plt.close('all')
            logger.info("‚úÖ Confusion matrix plot test passed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Confusion matrix plot test failed: {e}")
        
        try:
            EvaluationVisualizer.plot_roc_curves(y_true, y_prob, class_names)
            plt.close('all')
            logger.info("‚úÖ ROC curves plot test passed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ROC curves plot test failed: {e}")
        
        logger.info("‚úÖ Evaluation visualizer test passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Evaluation visualizer test failed: {e}")
        return False


def main():
    """Run all evaluation tests."""
    logger.info("üß™ Testing evaluation components...")
    
    tests = [
        ("Evaluation Imports", test_evaluation_imports),
        ("Clinical Metrics", test_clinical_metrics),
        ("Model Evaluator", test_model_evaluator),
        ("Cross Validation", test_cross_validation),
        ("Robustness Testing", test_robustness_testing),
        ("Performance Metrics", test_performance_metrics),
        ("Bias Detection", test_bias_detection),
        ("Evaluation Visualizer", test_evaluation_visualizer),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        logger.info(f"\nüß™ Testing {test_name}...")
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            logger.error(f"‚ùå {test_name} test failed with exception: {e}")
            failed += 1
    
    logger.info(f"\nüìä Evaluation Test Results:")
    logger.info(f"‚úÖ Passed: {passed}")
    logger.info(f"‚ùå Failed: {failed}")
    
    if failed == 0:
        logger.info("‚úÖ All evaluation tests passed!")
    else:
        logger.warning(f"‚ö†Ô∏è {failed} evaluation tests failed!")
    
    return failed == 0


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 